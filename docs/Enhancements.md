# Enhancements and Planned Features

> **Note:** This file only tracks planned or in-progress enhancements and features. Completed items are moved to CHANGELOG.md. Regularly reconcile this file with CHANGELOG.md to maintain compliance.

---

## **âœ… RESOLVED: Empty Statistics Issue (Issue #99)**

**Status**: COMPLETED - Moved to CHANGELOG.md

The `get_dshield_statistics` tool has been fixed with:
- Dynamic index discovery instead of hardcoded patterns
- New `diagnose_data_availability` diagnostic tool
- Enhanced error handling and logging
- Proper configuration with fallback support

**See CHANGELOG.md for complete implementation details.**

---

## **âœ… COMPLETED: Graceful Degradation Implementation (Issue #60)**

**Status**: COMPLETED - Moved to CHANGELOG.md

The graceful degradation system has been successfully implemented with:
- Real health checks for all dependencies (Elasticsearch, DShield API, LaTeX, Threat Intelligence)
- Timeout protection to prevent hanging health checks
- Dynamic feature management based on dependency health
- Dynamic tool registration that only exposes functional tools
- Full MCP protocol compliance with graceful degradation
- Comprehensive logging and status reporting

**See CHANGELOG.md for complete implementation details.**

**Key Results**:
- **Feature Availability**: 12/12 (100%) - All core features available
- **Tool Availability**: 29/33 (88%) - 4 tools properly disabled due to missing dependencies
- **Server Startup**: Under 30 seconds with timeout protection
- **Graceful Degradation**: System continues to function with reduced but stable functionality

---

## **ðŸ”„ IN PROGRESS: JSON-RPC Error Handling Implementation (Issue #58)**

**Status**: IN PROGRESS - Implementation planning phase

**Priority**: Critical - Must be resolved before production deployment

**Description**: Implement proper JSON-RPC error handling with correct error codes and structured error responses to ensure full MCP protocol compliance.

**Implementation Plan**: See `docs/Implementation_Docs/ISSUE_58_JSON_RPC_ERROR_HANDLING_IMPLEMENTATION.md`

**Current Phase**: Phase 1 - Core Error Handling Infrastructure âœ… COMPLETED
**Next Steps**: 
1. âœ… Create MCPErrorHandler class - COMPLETED
2. âœ… Update user configuration with error handling settings - COMPLETED
3. âœ… Update configuration loader - COMPLETED
4. âœ… Implement basic error handling in tool call handler - COMPLETED
5. âœ… Update Elasticsearch client error handling - COMPLETED
6. âœ… Update DShield client error handling - COMPLETED
7. âœ… Update LaTeX tools error handling - COMPLETED
8. âœ… Fix all failing tests - COMPLETED
9. âœ… Add timeout handling to all tool calls - COMPLETED
10. âœ… Integrate MCPErrorHandler into main MCP server - COMPLETED

**Testing Approach**: 
- âœ… All Phase 2 components have comprehensive error handling tests
- âœ… All tests are now passing (22/22 error handling tests)
- âœ… Phase 3: Timeout handling added to all tool calls
- ðŸ“‹ Testing guidelines documented in implementation plan
- ðŸ”§ DShield Client test fixes: 6/8 tests passing (patch path issues resolved)

**Dependencies**: None - can proceed immediately

---

## **Implementation Priority and Dependencies**

1. **Phase 1 (Week 1) - CRITICAL** âœ…:
   - âœ… Implement proper JSON-RPC error handling (Issue #58)
   - âœ… Create MCPErrorHandler class
   - âœ… Update tool call handler with error handling
   - âœ… Add timeout and retry logic

2. **Phase 2 (Week 2) - CRITICAL**:
   - âœ… Update Elasticsearch client error handling - COMPLETED
   - âœ… Update DShield client error handling - COMPLETED
   - ðŸ”„ Update LaTeX tools error handling - IN PROGRESS

2. **Phase 2 (Week 2-3)**:
   - Implement `detect_statistical_anomalies` tool
   - Implement `get_intelligent_summary` tool
   - Add aggregation-based data retrieval methods

3. **Phase 3 (Week 4-5)**:
   - Implement `analyze_behavioral_patterns` tool
   - Implement `get_risk_weighted_sample` tool
   - Add clustering and risk scoring algorithms

4. **Phase 4 (Week 6)**:
   - Implement `analyze_time_series_trends` tool
   - Add forecasting capabilities
   - Complete integration testing

---

## **âœ… COMPLETED: Statistical Anomaly Detection Tool (Issue #100)**

**Status**: COMPLETED - Moved to CHANGELOG.md

The `detect_statistical_anomalies` tool has been implemented with:
- Multiple detection methods: Z-score, IQR, Isolation Forest, and Time Series analysis
- Aggregation-based approach to avoid context flooding
- Risk assessment and actionable recommendations
- Comprehensive testing and documentation
- Integration with existing MCP server infrastructure

**See CHANGELOG.md for complete implementation details.**

**Tool Definition**:
```python
Tool(
    name="detect_statistical_anomalies",
    description="Detect statistical anomalies in DShield data patterns",
    inputSchema={
        "type": "object",
        "properties": {
            "time_range_hours": {"type": "integer", "default": 168},
            "anomaly_threshold": {"type": "number", "default": 2.0},
            "metrics": {
                "type": "array",
                "items": {"type": "string"},
                "default": ["event_count", "unique_ips", "geographic_spread"]
            }
        }
    }
)
```

**Implementation Plan**:
```python
async def _detect_statistical_anomalies(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Detect statistical anomalies in events without returning all raw data."""
    time_range_hours = arguments.get("time_range_hours", 24)
    anomaly_methods = arguments.get("anomaly_methods", ["zscore", "iqr"])
    sensitivity = arguments.get("sensitivity", 2.5)
    dimensions = arguments.get("dimensions", ["source_ip", "destination_port", "bytes_transferred", "event_rate"])
    return_summary_only = arguments.get("return_summary_only", True)
    max_anomalies = arguments.get("max_anomalies", 50)
    
    try:
        # Use aggregation queries instead of fetching raw events
        anomaly_data = await self._get_anomaly_aggregations(
            time_range_hours, dimensions, anomaly_methods, sensitivity
        )
        
        # Apply statistical methods server-side
        anomalies = await self._apply_anomaly_detection_methods(
            anomaly_data, anomaly_methods, sensitivity, max_anomalies
        )
        
        # Generate risk assessment and recommendations
        anomalies["risk_assessment"] = await self._assess_anomaly_risk(anomalies)
        anomalies["recommended_actions"] = await self._generate_anomaly_recommendations(anomalies)
        
        return [{
            "type": "text",
            "text": json.dumps(anomalies, indent=2, default=str)
        }]
        
    except Exception as e:
        logger.error("Anomaly detection failed", error=str(e))
        return [{
            "type": "text",
            "text": f"Anomaly detection failed: {str(e)}"
        }]

async def _get_anomaly_aggregations(self, time_range_hours: int, dimensions: List[str], 
                                   methods: List[str], sensitivity: float) -> Dict[str, Any]:
    """Get aggregated data for anomaly detection without raw events."""
    # Build aggregation query for each dimension
    aggs = {}
    
    for dimension in dimensions:
        if dimension in ["source_ip", "destination_ip"]:
            aggs[f"{dimension}_counts"] = {
                "terms": {"field": f"{dimension}.keyword", "size": 1000}
            }
        elif dimension in ["destination_port", "bytes_transferred"]:
            aggs[f"{dimension}_stats"] = {
                "stats": {"field": dimension}
            }
        elif dimension == "event_rate":
            aggs[f"{dimension}_time_series"] = {
                "date_histogram": {
                    "field": "@timestamp",
                    "calendar_interval": "1h"
                }
            }
    
    # Execute aggregation query
    query_body = {
        "size": 0,
        "query": {
            "range": {
                "@timestamp": {
                    "gte": f"now-{time_range_hours}h",
                    "lte": "now"
                }
            }
        },
        "aggs": aggs
    }
    
    result = await self.elastic_client.client.search(
        index=await self.elastic_client.get_available_indices(),
        body=query_body
    )
    
    return result["aggregations"]
```

**Implementation approach**:
- Z-score analysis for numerical fields
- IQR (Interquartile Range) for outlier detection
- Time-series decomposition for temporal anomalies
- Return only the anomaly metadata, not the full events

---

## **3. Intelligent Event Summarization Tool: `get_intelligent_summary`**

**Purpose**: Get AI-preprocessed summary of security events with drill-down capabilities
**Priority**: HIGH (context efficiency)

**Tool Definition**:
```python
Tool(
    name="get_intelligent_summary",
    description="Get AI-preprocessed summary of security events with drill-down capabilities",
    inputSchema={
        "type": "object",
        "properties": {
            "time_range_hours": {"type": "integer", "default": 24},
            "summary_depth": {
                "type": "string",
                "enum": ["executive", "tactical", "operational"],
                "default": "tactical",
                "description": "Level of detail in summary"
            },
            "focus_areas": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Areas to focus on: threats, vulnerabilities, campaigns, geography, ports"
            },
            "risk_threshold": {
                "type": "string",
                "enum": ["critical", "high", "medium", "low"],
                "default": "medium",
                "description": "Minimum risk level to include"
            },
            "include_recommendations": {
                "type": "boolean",
                "default": true,
                "description": "Include actionable recommendations"
            }
        }
    }
)
```

**Implementation Plan**:
```python
async def _get_intelligent_summary(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Get AI-preprocessed summary of security events with drill-down capabilities."""
    time_range_hours = arguments.get("time_range_hours", 24)
    summary_depth = arguments.get("summary_depth", "tactical")
    focus_areas = arguments.get("focus_areas", ["threats", "vulnerabilities", "campaigns"])
    risk_threshold = arguments.get("risk_threshold", "medium")
    include_recommendations = arguments.get("include_recommendations", True)
    
    try:
        # Get high-level aggregations for summary
        summary_data = await self._get_summary_aggregations(
            time_range_hours, focus_areas, risk_threshold
        )
        
        # Generate intelligent insights
        insights = await self._generate_intelligent_insights(
            summary_data, summary_depth, focus_areas
        )
        
        # Create drill-down paths
        drill_down_paths = await self._create_drill_down_paths(
            summary_data, insights, summary_depth
        )
        
        # Generate recommendations if requested
        recommendations = []
        if include_recommendations:
            recommendations = await self._generate_actionable_recommendations(
                insights, risk_threshold
            )
        
        summary = {
            "time_range_hours": time_range_hours,
            "summary_depth": summary_depth,
            "focus_areas": focus_areas,
            "risk_threshold": risk_threshold,
            "timestamp": datetime.utcnow().isoformat(),
            "executive_summary": insights["executive"],
            "tactical_details": insights["tactical"],
            "operational_insights": insights["operational"],
            "drill_down_paths": drill_down_paths,
            "recommendations": recommendations,
            "risk_assessment": insights["risk_assessment"]
        }
        
        return [{
            "type": "text",
            "text": json.dumps(summary, indent=2, default=str)
        }]
        
    except Exception as e:
        logger.error("Intelligent summary generation failed", error=str(e))
        return [{
            "type": "text",
            "text": f"Summary generation failed: {str(e)}"
        }]
```

---

## **4. Behavioral Pattern Clustering Tool: `analyze_behavioral_patterns`**

**Purpose**: Cluster events by behavior patterns and return only unique patterns
**Priority**: MEDIUM (advanced analysis)

**Tool Definition**:
```python
Tool(
    name="analyze_behavioral_patterns",
    description="Cluster events by behavior patterns and return only unique patterns",
    inputSchema={
        "type": "object",
        "properties": {
            "time_range_hours": {"type": "integer", "default": 24},
            "clustering_algorithm": {
                "type": "string",
                "enum": ["dbscan", "kmeans", "hierarchical", "optics"],
                "default": "dbscan",
                "description": "Clustering algorithm to use"
            },
            "min_cluster_size": {
                "type": "integer",
                "default": 5,
                "description": "Minimum events to form a cluster"
            },
            "pattern_features": {
                "type": "array",
                "items": {"type": "string"},
                "default": ["temporal", "volumetric", "geographic", "service"],
                "description": "Features to use for pattern detection"
            },
            "return_representatives_only": {
                "type": "boolean",
                "default": true,
                "description": "Return only cluster representatives, not all events"
            }
        }
    }
)
```

**Implementation Plan**:
```python
async def _analyze_behavioral_patterns(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Cluster events by behavior patterns and return only unique patterns."""
    time_range_hours = arguments.get("time_range_hours", 24)
    clustering_algorithm = arguments.get("clustering_algorithm", "dbscan")
    min_cluster_size = arguments.get("min_cluster_size", 5)
    pattern_features = arguments.get("pattern_features", ["temporal", "volumetric", "geographic", "service"])
    return_representatives_only = arguments.get("return_representatives_only", True)
    
    try:
        # Extract behavioral features using aggregations
        behavioral_features = await self._extract_behavioral_features(
            time_range_hours, pattern_features
        )
        
        # Apply clustering algorithm
        clusters = await self._apply_clustering_algorithm(
            behavioral_features, clustering_algorithm, min_cluster_size
        )
        
        # Generate pattern representatives
        pattern_representatives = await self._generate_pattern_representatives(
            clusters, behavioral_features, return_representatives_only
        )
        
        # Analyze cluster characteristics
        cluster_analysis = await self._analyze_cluster_characteristics(
            clusters, behavioral_features
        )
        
        result = {
            "time_range_hours": time_range_hours,
            "clustering_algorithm": clustering_algorithm,
            "min_cluster_size": min_cluster_size,
            "pattern_features": pattern_features,
            "total_clusters": len(clusters),
            "pattern_representatives": pattern_representatives,
            "cluster_analysis": cluster_analysis,
            "behavioral_insights": await self._extract_behavioral_insights(clusters)
        }
        
        return [{
            "type": "text",
            "text": json.dumps(result, indent=2, default=str)
        }]
        
    except Exception as e:
        logger.error("Behavioral pattern analysis failed", error=str(e))
        return [{
            "type": "text",
            "text": f"Pattern analysis failed: {str(e)}"
        }]
```

---

## **5. Risk-Weighted Event Sampler Tool: `get_risk_weighted_sample`**

**Purpose**: Return a risk-weighted sample of events prioritizing high-value intelligence
**Priority**: MEDIUM (intelligence prioritization)

**Tool Definition**:
```python
Tool(
    name="get_risk_weighted_sample",
    description="Return a risk-weighted sample of events prioritizing high-value intelligence",
    inputSchema={
        "type": "object",
        "properties": {
            "time_range_hours": {"type": "integer", "default": 24},
            "sample_size": {
                "type": "integer",
                "default": 100,
                "description": "Target sample size"
            },
            "risk_factors": {
                "type": "object",
                "properties": {
                    "known_bad_ips": {"type": "number", "default": 10.0},
                    "zero_day_indicators": {"type": "number", "default": 8.0},
                    "lateral_movement": {"type": "number", "default": 7.0},
                    "data_exfiltration": {"type": "number", "default": 9.0},
                    "persistence_mechanisms": {"type": "number", "default": 6.0}
                },
                "description": "Weights for different risk factors"
            },
            "diversity_factor": {
                "type": "number",
                "default": 0.3,
                "description": "Balance between risk and diversity (0-1)"
            }
        }
    }
)
```

**Implementation Plan**:
```python
async def _get_risk_weighted_sample(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Return a risk-weighted sample of events prioritizing high-value intelligence."""
    time_range_hours = arguments.get("time_range_hours", 24)
    sample_size = arguments.get("sample_size", 100)
    risk_factors = arguments.get("risk_factors", {
        "known_bad_ips": 10.0,
        "zero_day_indicators": 8.0,
        "lateral_movement": 7.0,
        "data_exfiltration": 9.0,
        "persistence_mechanisms": 6.0
    })
    diversity_factor = arguments.get("diversity_factor", 0.3)
    
    try:
        # Get risk-scored events using aggregations
        risk_scored_events = await self._get_risk_scored_events(
            time_range_hours, risk_factors
        )
        
        # Apply diversity sampling
        diverse_sample = await self._apply_diversity_sampling(
            risk_scored_events, sample_size, diversity_factor
        )
        
        # Generate sample summary
        sample_summary = await self._generate_sample_summary(
            diverse_sample, risk_factors, diversity_factor
        )
        
        result = {
            "time_range_hours": time_range_hours,
            "sample_size": len(diverse_sample),
            "risk_factors": risk_factors,
            "diversity_factor": diversity_factor,
            "sample_summary": sample_summary,
            "risk_distribution": await self._analyze_risk_distribution(diverse_sample),
            "diversity_metrics": await self._calculate_diversity_metrics(diverse_sample),
            "recommended_focus_areas": await self._identify_focus_areas(diverse_sample)
        }
        
        return [{
            "type": "text",
            "text": json.dumps(result, indent=2, default=str)
        }]
        
    except Exception as e:
        logger.error("Risk-weighted sampling failed", error=str(e))
        return [{
            "type": "text",
            "text": f"Risk sampling failed: {str(e)}"
        }]
```

---

## **6. Time-Series Trend Analyzer Tool: `analyze_time_series_trends`**

**Purpose**: Detect trends, seasonality, and change points without returning raw data
**Priority**: MEDIUM (trend analysis)

**Tool Definition**:
```python
Tool(
    name="analyze_time_series_trends",
    description="Detect trends, seasonality, and change points without returning raw data",
    inputSchema={
        "type": "object",
        "properties": {
            "time_range_hours": {"type": "integer", "default": 168},
            "granularity": {
                "type": "string",
                "enum": ["minute", "hour", "day"],
                "default": "hour"
            },
            "trend_metrics": {
                "type": "array",
                "items": {"type": "string"},
                "default": ["event_volume", "unique_sources", "attack_severity", "geographic_spread"]
            },
            "detect_change_points": {
                "type": "boolean",
                "default": true,
                "description": "Detect significant change points"
            },
            "forecast_hours": {
                "type": "integer",
                "default": 0,
                "description": "Hours to forecast ahead (0 = no forecast)"
            }
        }
    }
)
```

**Implementation Plan**:
```python
async def _analyze_time_series_trends(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Detect trends, seasonality, and change points without returning raw data."""
    time_range_hours = arguments.get("time_range_hours", 168)
    granularity = arguments.get("granularity", "hour")
    trend_metrics = arguments.get("trend_metrics", ["event_volume", "unique_sources", "attack_severity", "geographic_spread"])
    detect_change_points = arguments.get("detect_change_points", True)
    forecast_hours = arguments.get("forecast_hours", 0)
    
    try:
        # Get time-series aggregations
        time_series_data = await self._get_time_series_aggregations(
            time_range_hours, granularity, trend_metrics
        )
        
        # Analyze trends for each metric
        trend_analysis = {}
        for metric in trend_metrics:
            metric_data = time_series_data.get(metric, [])
            if metric_data:
                trend_analysis[metric] = await self._analyze_single_metric_trends(
                    metric_data, detect_change_points, forecast_hours
                )
        
        # Detect cross-metric correlations
        correlations = await self._detect_cross_metric_correlations(trend_analysis)
        
        # Generate change point analysis
        change_points = {}
        if detect_change_points:
            change_points = await self._detect_significant_change_points(trend_analysis)
        
        # Generate forecasts if requested
        forecasts = {}
        if forecast_hours > 0:
            forecasts = await self._generate_forecasts(trend_analysis, forecast_hours)
        
        result = {
            "time_range_hours": time_range_hours,
            "granularity": granularity,
            "trend_metrics": trend_metrics,
            "trend_analysis": trend_analysis,
            "cross_metric_correlations": correlations,
            "change_points": change_points,
            "forecasts": forecasts,
            "trend_summary": await self._generate_trend_summary(trend_analysis),
            "recommendations": await self._generate_trend_recommendations(trend_analysis)
        }
        
        return [{
            "type": "text",
            "text": json.dumps(result, indent=2, default=str)
        }]
        
    except Exception as e:
        logger.error("Time-series trend analysis failed", error=str(e))
        return [{
            "type": "text",
            "text": f"Trend analysis failed: {str(e)}"
        }]
```

---

## **Key Benefits of This Approach**

1. **Context Preservation**: Tools return 100-200 lines of actionable intelligence instead of 10k+ raw events
2. **Computational Efficiency**: Heavy lifting done server-side using Elasticsearch aggregations
3. **Progressive Disclosure**: Start with summaries, drill down only when needed
4. **Risk-Based Prioritization**: Focus on what matters most
5. **Pattern Recognition**: Identify trends and anomalies that would be missed in raw data
6. **Actionable Intelligence**: Provide recommendations, not just data

---

## **Next Steps**

1. **Immediate**: Implement the diagnostic tool to identify the statistics issue
2. **Short-term**: Fix the index pattern configuration in the Elasticsearch client
3. **Medium-term**: Implement the first two enhancement tools
4. **Long-term**: Complete the full suite of analysis tools

---

## **Implementation Example for Statistical Anomaly Detection**

```python
async def _detect_statistical_anomalies(self, arguments: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Detect statistical anomalies in events without returning all raw data."""
    import numpy as np
    from scipy import stats
    from sklearn.ensemble import IsolationForest
    
    time_range_hours = arguments.get("time_range_hours", 24)
    anomaly_methods = arguments.get("anomaly_methods", ["zscore", "iqr"])
    sensitivity = arguments.get("sensitivity", 2.5)
    dimensions = arguments.get("dimensions", ["source_ip", "destination_port", "bytes_transferred", "event_rate"])
    return_summary_only = arguments.get("return_summary_only", True)
    max_anomalies = arguments.get("max_anomalies", 50)
    
    # Query all events (server-side)
    events, total_count, _ = await self.elastic_client.query_dshield_events(
        time_range_hours=time_range_hours,
        page_size=10000  # Process large batches server-side
    )
    
    if not events:
        return [{
            "type": "text",
            "text": "No events found for anomaly detection"
        }]
    
    anomalies = {
        "summary": {
            "total_events_analyzed": total_count,
            "time_range_hours": time_range_hours,
            "methods_used": anomaly_methods,
            "sensitivity": sensitivity
        },
        "anomalies_by_method": {},
        "top_anomalies": [],
        "patterns": {}
    }
    
    # Z-score anomaly detection for numerical fields
    if "zscore" in anomaly_methods:
        numerical_data = self._extract_numerical_features(events, dimensions)
        z_scores = np.abs(stats.zscore(numerical_data))
        anomaly_indices = np.where(z_scores > sensitivity)[0]
        
        anomalies["anomalies_by_method"]["zscore"] = {
            "count": len(anomaly_indices),
            "percentage": (len(anomaly_indices) / total_count) * 100,
            "top_dimensions": self._get_top_anomaly_dimensions(z_scores, dimensions)
        }
    
    # IQR-based outlier detection
    if "iqr" in anomaly_methods:
        for dim in dimensions:
            values = self._extract_dimension_values(events, dim)
            Q1 = np.percentile(values, 25)
            Q3 = np.percentile(values, 75)
            IQR = Q3 - Q1
            outliers = [v for v in values if v < (Q1 - 1.5 * IQR) or v > (Q3 + 1.5 * IQR)]
            
            if outliers:
                anomalies["anomalies_by_method"].setdefault("iqr", {})[dim] = {
                    "outlier_count": len(outliers),
                    "outlier_range": [min(outliers), max(outliers)],
                    "normal_range": [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]
                }
    
    # Isolation Forest for multivariate anomalies
    if "isolation_forest" in anomaly_methods:
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        features = self._prepare_features_for_ml(events, dimensions)
        predictions = iso_forest.fit_predict(features)
        anomaly_count = np.sum(predictions == -1)
        
        anomalies["anomalies_by_method"]["isolation_forest"] = {
            "anomaly_count": int(anomaly_count),
            "anomaly_percentage": (anomaly_count / len(events)) * 100,
            "anomaly_scores": iso_forest.score_samples(features[:max_anomalies]).tolist()
        }
    
    # Time series anomaly detection
    if "time_series" in anomaly_methods:
        time_series_anomalies = self._detect_time_series_anomalies(events, sensitivity)
        anomalies["anomalies_by_method"]["time_series"] = time_series_anomalies
    
    # Aggregate top anomalies across all methods
    if not return_summary_only:
        top_anomalous_events = self._get_top_anomalous_events(
            events, anomalies["anomalies_by_method"], max_anomalies
        )
        anomalies["top_anomalies"] = top_anomalous_events
    
    # Pattern detection in anomalies
    anomalies["patterns"] = {
        "temporal_clustering": self._detect_temporal_clustering(anomalies),
        "source_concentration": self._detect_source_concentration(anomalies),
        "target_patterns": self._detect_target_patterns(anomalies)
    }
    
    # Risk scoring
    anomalies["risk_assessment"] = {
        "overall_risk": self._calculate_overall_risk(anomalies),
        "recommended_actions": self._generate_recommendations(anomalies)
    }
    
    return [{
        "type": "text",
        "text": json.dumps(anomalies, indent=2, default=str)
    }]
```

**Benefits of This Approach**:

- **Context Preservation**: Instead of sending 10k+ events, send ~100-200 lines of actionable intelligence
- **Computational Efficiency**: Heavy lifting done server-side where resources are available
- **Progressive Disclosure**: Start with summaries, drill down only when needed
- **Risk-Based Prioritization**: Focus on what matters most
- **Pattern Recognition**: Identify trends and anomalies that would be missed in raw data
- **Actionable Intelligence**: Provide recommendations, not just data

---

## **Quick Fix for the Empty Statistics Issue**

For the immediate `get_dshield_statistics` returning empty results issue, add a diagnostic tool:

```python
Tool(
    name="diagnose_data_availability",
    description="Diagnose why queries return empty results",
    inputSchema={
        "type": "object",
        "properties": {
            "check_indices": {"type": "boolean", "default": true},
            "check_mappings": {"type": "boolean", "default": true},
            "check_recent_data": {"type": "boolean", "default": true},
            "sample_query": {"type": "boolean", "default": true}
        }
    }
)
```

This will help identify whether it's an index naming issue, time range problem, or field mapping mismatch without flooding the context with data.